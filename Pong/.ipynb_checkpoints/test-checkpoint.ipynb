{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of [Deep Q-Network](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), a deep learning variant of the Q-learning algorithm for machine learning, developed by [Google DeepMind](https://deepmind.com/). A very good explanation of Reinforcement Learning, Q-Networks and particular Deep Q-Network can be found [here](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287).  \n",
    "The implemention is inspired and partially based on [this one](https://github.com/jaidmin/pytorch-q-learning). Besides I used some ideas from [this repository](https://github.com/Shmuma/ptan/tree/master/samples/dqn_speedup) and Atari wrappers from [OpenAI Baselines repositories](https://github.com/openai/baselines/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print('Use GPU: {}'.format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory():\n",
    "    '''\n",
    "    Replay memory to store states, actions, rewards, dones for batch sampling\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        :param capacity: replay memory capacity\n",
    "        '''\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, done, next_state):\n",
    "        '''\n",
    "        :param state: current state, atari_wrappers.LazyFrames object\n",
    "        :param action: action\n",
    "        :param reward: reward for the action\n",
    "        :param done: \"done\" flag is True when the episode finished\n",
    "        :param next_state: next state, atari_wrappers.LazyFrames object\n",
    "        '''\n",
    "        experience = (state, action, reward, done, next_state)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Samples the data from the buffer of a desired size\n",
    "        \n",
    "        :param batch_size: sample batch size\n",
    "        :return: batch of (states, actions, rewards, dones, next states).\n",
    "                 all are numpy arrays. states and next states have shape of \n",
    "                 (batch_size, frames, width, height), where frames = 4.\n",
    "                 actions, rewards and dones have shape of (batch_size,)\n",
    "        '''\n",
    "        if self.count() < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            \n",
    "        state_batch = np.array([np.array(experience[0]) for experience in batch])\n",
    "        action_batch = np.array([experience[1] for experience in batch])\n",
    "        reward_batch = np.array([experience[2] for experience in batch])\n",
    "        done_batch = np.array([experience[3] for experience in batch])\n",
    "        next_state_batch = np.array([np.array(experience[4]) for experience in batch])\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, done_batch, next_state_batch\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "    Deep Q-Network\n",
    "    '''\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)\n",
    "        # self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        # self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Forward propogation\n",
    "        \n",
    "        :param inputs: images. expected sshape is (batch_size, frames, width, height)\n",
    "        '''\n",
    "        out = F.relu(self.conv1(inputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from atari_wrappers import wrap_dqn\n",
    "import datetime\n",
    "\n",
    "class PongAgent:\n",
    "    '''\n",
    "    Pong agent. Implements training and testing methods\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.env = wrap_dqn(gym.make('PongDeterministic-v0'))\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        \n",
    "        self.dqn = DQN(self.num_actions)\n",
    "        self.target_dqn = DQN(self.num_actions)\n",
    "        \n",
    "        if use_gpu:\n",
    "            self.dqn.cuda()\n",
    "            self.target_dqn.cuda()        \n",
    "        \n",
    "        self.buffer = ReplayMemory(1000000)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optim = optim.RMSprop(self.dqn.parameters(), lr=0.0001)\n",
    "        \n",
    "        self.out_dir = './model'\n",
    "        \n",
    "        if not os.path.exists(self.out_dir):\n",
    "            os.makedirs(self.out_dir)\n",
    "\n",
    "        \n",
    "    def to_var(self, x):\n",
    "        '''\n",
    "        Converts x to Variable\n",
    "        \n",
    "        :param x: torch Tensor\n",
    "        :return: torch Variable\n",
    "        '''\n",
    "        x_var = Variable(x)\n",
    "        if use_gpu:\n",
    "            x_var = x_var.cuda()\n",
    "        return x_var\n",
    "\n",
    "        \n",
    "    def predict_q_values(self, states):\n",
    "        '''\n",
    "        Compute Q values bypassing states through estimation network\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :return: actions, Variable, the shape is (batch_size, num_actions)\n",
    "        '''\n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.dqn(states)\n",
    "        return actions\n",
    "\n",
    "    \n",
    "    def predict_q_target_values(self, states):\n",
    "        '''\n",
    "        Compute Q values bypassing states through target network\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :return: actions, Variable, the shape is (batch_size, num_actions)\n",
    "        '''\n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.target_dqn(states)\n",
    "        return actions\n",
    "\n",
    "    \n",
    "    def select_action(self, state, epsilon):\n",
    "        '''\n",
    "        Select action according to epsilon greedy policy. We will sometimes use \n",
    "        our model for choosing the action, and sometimes we will just sample one \n",
    "        uniformly.\n",
    "        \n",
    "        :param state: state, atari_wrappers.LazyFrames object - list of 4 frames,\n",
    "                      each is a shape of (1, width, height)\n",
    "        :param epsilon: epsilon for making choice between random and generated by dqn action\n",
    "        \n",
    "        :return: action index\n",
    "        '''\n",
    "        choice = np.random.choice([0, 1], p=(epsilon, (1 - epsilon)))\n",
    "        \n",
    "        if choice == 0:\n",
    "            return np.random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            actions = self.predict_q_values(state)\n",
    "            return np.argmax(actions.data.cpu().numpy())\n",
    "\n",
    "        \n",
    "    def update(self, states, targets, actions):\n",
    "        '''\n",
    "        Compute loss and do a backward propogation\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :param targets: actions from target network, numpy array the shape is (batch_size)\n",
    "        :param actions: actions, numpy array, the shape is (batch_size)\n",
    "        '''\n",
    "        targets = self.to_var(torch.unsqueeze(torch.from_numpy(targets).float(), -1))\n",
    "        actions = self.to_var(torch.unsqueeze(torch.from_numpy(actions).long(), -1))\n",
    "        \n",
    "        predicted_values = self.predict_q_values(states)\n",
    "        affected_values = torch.gather(predicted_values, 1, actions)\n",
    "        loss = self.mse_loss(affected_values, targets)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        \n",
    "    def get_epsilon(self, total_steps, max_epsilon_steps, epsilon_start, epsilon_final):\n",
    "        '''\n",
    "        Calculate epsilon value. It cannot be more than epsilon_start and less\n",
    "        than epsilon final. It is decayed with each step\n",
    "        \n",
    "        :param total_steps: total number of step from the training begin\n",
    "        :param max_epsilon_steps: maximum number of epsilon steps\n",
    "        :param epsilon_start: start epsilon value, e.g. 1\n",
    "        :param epsilon_final: final epsilon value, effectively a limit\n",
    "        :return: calculated epsilon value\n",
    "        '''\n",
    "        return max(epsilon_final, epsilon_start - total_steps / max_epsilon_steps)\n",
    "\n",
    "    \n",
    "    def sync_target_network(self):\n",
    "        '''\n",
    "        Copies weights from estimation to target network\n",
    "        '''\n",
    "        primary_params = list(self.dqn.parameters())\n",
    "        target_params = list(self.target_dqn.parameters())\n",
    "        for i in range(0, len(primary_params)):\n",
    "            target_params[i].data[:] = primary_params[i].data[:]\n",
    "            \n",
    "            \n",
    "    def calculate_q_targets(self, next_states, rewards, dones):\n",
    "        '''\n",
    "        Calculates Q-targets (actions from the target network)\n",
    "        \n",
    "        :param next_states: next states, numpy array, shape is (batch_size, frames, width, height)\n",
    "        :param rewards: rewards, numpy array, shape is (batch_size,)\n",
    "        :param dones: dones, numpy array, shape is (batch_size,)\n",
    "        '''\n",
    "        dones_mask = (dones == 1)\n",
    "        \n",
    "        predicted_q_target_values = self.predict_q_target_values(next_states)\n",
    "        \n",
    "        next_max_q_values = np.max(predicted_q_target_values.data.cpu().numpy(), axis=1)\n",
    "        next_max_q_values[dones_mask] = 0 # no next max Q values if the game is over\n",
    "        q_targets = rewards + self.gamma * next_max_q_values\n",
    "        \n",
    "        return q_targets\n",
    "    \n",
    "    \n",
    "    def save_final_model(self):\n",
    "        '''\n",
    "        Saves final model to the disk\n",
    "        '''\n",
    "        filename = '{}/final_model.pth'.format(self.out_dir)\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "        \n",
    "\n",
    "    def save_model_during_training(self, episode):\n",
    "        '''\n",
    "        Saves temporary models to the disk during training\n",
    "        \n",
    "        :param episode: episode number\n",
    "        '''\n",
    "        filename = '{}/current_model_{}.pth'.format(self.out_dir, episode)\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "        \n",
    "        \n",
    "    def load_model(self, filename):\n",
    "        '''\n",
    "        Loads model from the disk\n",
    "        \n",
    "        :param filename: model filename\n",
    "        '''\n",
    "        self.dqn.load_state_dict(torch.load(filename))\n",
    "        self.sync_target_network()\n",
    "        \n",
    "        \n",
    "    def play(self, episodes):\n",
    "        '''\n",
    "        Plays the game and renders it\n",
    "        \n",
    "        :param episodes: number of episodes to play\n",
    "        '''\n",
    "        for i in range(1, episodes + 1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "#                 action = np.random.choice(range(self.num_actions)) #playing randomly\n",
    "                action = self.select_action(state, 0) # force to choose an action from the network\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                self.env.render()\n",
    "                \n",
    "                \n",
    "    def close_env(self):\n",
    "        '''\n",
    "        Closes the environment. Should be called to clean-up\n",
    "        '''\n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def train(self, replay_buffer_fill_len, batch_size, episodes, stop_reward,\n",
    "              max_epsilon_steps, epsilon_start, epsilon_final, sync_target_net_freq):\n",
    "        '''\n",
    "        Trains the network\n",
    "        \n",
    "        :param replay_buffer_fill_len: how many elements should replay buffer contain\n",
    "                                       before training start\n",
    "        :param batch_size: batch size\n",
    "        :param episodes: how many episodes (max. value) to iterate\n",
    "        :param stop_reward: running reward value to be reached. upon reaching that\n",
    "                            value the training is stoped\n",
    "        :param max_epsilon_steps: maximum number of epsilon steps\n",
    "        :param epsilon_start: start epsilon value, e.g. 1\n",
    "        :param epsilon_final: final epsilon value, effectively a limit\n",
    "        :param sync_target_net_freq: how often to sync estimation and target networks\n",
    "        '''\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('Start training at: '+ time.asctime(time.localtime(start_time)))\n",
    "        \n",
    "        total_steps = 0\n",
    "        running_episode_reward = 0\n",
    "        \n",
    "        # populate replay memory\n",
    "        print('Populating replay buffer... ')\n",
    "        print('\\n')\n",
    "        state = self.env.reset()\n",
    "        for i in range(replay_buffer_fill_len):\n",
    "            action = self.select_action(state, 1) # force to choose a random action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            self.buffer.add(state, action, reward, done, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                \n",
    "        print('replay buffer populated with {} transitions, start training...'.format(self.buffer.count()))\n",
    "        print('\\n')\n",
    "        \n",
    "        # main loop - iterate over episodes\n",
    "        for i in range(1, episodes + 1):\n",
    "            # reset the environment\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # reset spisode reward and length\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            # play until it is possible\n",
    "            while not done:\n",
    "                # synchronize target network with estimation network in required frequence\n",
    "                if (total_steps % sync_target_net_freq) == 0:\n",
    "                    print('synchronizing target network...')\n",
    "                    print('\\n')\n",
    "                    self.sync_target_network()\n",
    "\n",
    "                # calculate epsilon and select greedy action\n",
    "                epsilon = self.get_epsilon(total_steps, max_epsilon_steps, epsilon_start, epsilon_final)\n",
    "                action = self.select_action(state, epsilon)\n",
    "                \n",
    "                # execute action in the environment\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.add(state, action, reward, done, next_state)\n",
    "                \n",
    "                # sample random minibatch of transactions\n",
    "                s_batch, a_batch, r_batch, d_batch, next_s_batch = self.buffer.sample(batch_size)\n",
    "                \n",
    "                # estimate Q value using the target network\n",
    "                q_targets = self.calculate_q_targets(next_s_batch, r_batch, d_batch)\n",
    "                \n",
    "                # update weights in the estimation network\n",
    "                self.update(s_batch, q_targets, a_batch)\n",
    "                \n",
    "                # set the state for the next action selction and update counters and reward\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                episode_length += 1\n",
    "                episode_reward += reward\n",
    "                \n",
    "            running_episode_reward = running_episode_reward * 0.9 + 0.1 * episode_reward\n",
    "\n",
    "            if (i % 10) == 0 or (running_episode_reward > stop_reward):\n",
    "                print('global step: {}'.format(total_steps))\n",
    "                print('episode: {}'.format(i))\n",
    "                print('running reward: {}'.format(round(running_episode_reward, 2)))\n",
    "                print('current epsilon: {}'.format(round(epsilon, 2)))\n",
    "                print('episode_length: {}'.format(episode_length))\n",
    "                print('episode reward: {}'.format(episode_reward))\n",
    "                print('\\n')\n",
    "                \n",
    "            if (i % 50) == 0 or (running_episode_reward > stop_reward):\n",
    "                curr_time = time.time()\n",
    "                print('current time: ' + time.asctime(time.localtime(curr_time)))\n",
    "                print('running for: ' + str(datetime.timedelta(seconds=curr_time - start_time)))\n",
    "                print('saving model after {} episodes...'.format(i))\n",
    "                print('\\n')\n",
    "                self.save_model_during_training(i)\n",
    "            \n",
    "            if running_episode_reward > stop_reward:\n",
    "                print('stop reward reached!')\n",
    "                print('saving final model...')\n",
    "                print('\\n')\n",
    "                self.save_final_model()\n",
    "                break\n",
    "        \n",
    "        print('Finish training at: '+ time.asctime(time.localtime(start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "agent = PongAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-23df98a784ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model_d_v0/current_model_1050.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-eb056847af5f>\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;31m#                 action = np.random.choice(range(self.num_actions)) #playing randomly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# force to choose an action from the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Transfer-Learning\\Pong\\atari_wrappers.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.load_model('./model_d_v0/current_model_1050.pth')\n",
    "agent.play(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.close_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
